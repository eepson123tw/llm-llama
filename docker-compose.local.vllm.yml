version: '3.10'

services:

 vllm:
    container_name: vllm
    image: vllm/vllm-openai:v0.6.3.post1
    restart: unless-stopped
    ports:
      - "8000:8000"
    ipc: host
    volumes: 
      - /mnt/llm:/app/model
    command: [
      "--model", "/app/model/Llama-3.2-11B-Vision-Instruct", 
      "--gpu-memory-utilization", "0.65",
      "--max-model-len", "16384",
      "--max-num-seqs","2",
      "--tensor-parallel-size", "2",
      "--quantization","fp8",
      "--enforce-eager",]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0','1']
              capabilities: [gpu]

